# ü¶ô –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Llama –≤ Backend

## üìã –û–±–∑–æ—Ä

–≠—Ç–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ–∫–∞–∂–µ—Ç, –∫–∞–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å Llama LLM –≤ –≤–∞—à Express –±–µ–∫–µ–Ω–¥ –¥–ª—è AI —á–∞—Ç–∞. –ï—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–∞—à–∏—Ö –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π.

## üéØ –í–∞—Ä–∏–∞–Ω—Ç—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

### –í–∞—Ä–∏–∞–Ω—Ç 1: Ollama –≤ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ (–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è) üê≥‚≠ê

**–°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–±** - –∑–∞–ø—É—Å—Ç–∏—Ç—å Ollama –≤ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ. –ù–µ —Ç—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–∞ —Å–∏—Å—Ç–µ–º—É –∏ –ª–µ–≥–∫–æ —É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è.

#### –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Docker

```bash
# macOS
brew install --cask docker

# –ò–ª–∏ —Å–∫–∞—á–∞–π—Ç–µ Docker Desktop —Å https://www.docker.com/products/docker-desktop
```

#### –®–∞–≥ 2: –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä

```bash
cd backendCashStream
docker-compose up -d
```

#### –®–∞–≥ 3: –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å

```bash
# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
docker exec -it cashstream-ollama ollama pull llama3.2:1b

# –ò–ª–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞:
docker exec -it cashstream-ollama ollama pull llama3.2:3b
```

#### –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–∞–±–æ—Ç—É

```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ Ollama —Ä–∞–±–æ—Ç–∞–µ—Ç
curl http://localhost:11434/api/tags
```

**–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:** –°–º. `DOCKER_SETUP.md`

---

### –í–∞—Ä–∏–∞–Ω—Ç 2: Ollama (–õ–æ–∫–∞–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞)

**Ollama** - —Å–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± –∑–∞–ø—É—Å—Ç–∏—Ç—å Llama –ª–æ–∫–∞–ª—å–Ω–æ. –û–Ω –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç REST API –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏–∑ –∫–æ—Ä–æ–±–∫–∏.

#### –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# –°–∫–∞—á–∞–π—Ç–µ —Å https://ollama.com/download
```

#### –®–∞–≥ 2: –ó–∞–ø—É—Å–∫ Ollama –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama (–æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ —Å–µ—Ä–≤–∏—Å)
ollama serve

# –í –¥—Ä—É–≥–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å
# –î–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø–æ–¥–æ–π–¥–µ—Ç –Ω–µ–±–æ–ª—å—à–∞—è –º–æ–¥–µ–ª—å:
ollama pull llama3.2:1b  # –û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è, ~1.3GB
# –∏–ª–∏
ollama pull llama3.2:3b  # –ë–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏/–∫–∞—á–µ—Å—Ç–≤–∞, ~2GB
# –∏–ª–∏
ollama pull mistral:7b  # –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, ~4.1GB
```

#### –®–∞–≥ 3: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤ –±–µ–∫–µ–Ω–¥

```bash
cd backendCashStream
yarn add axios
```

#### –®–∞–≥ 4: –°–æ–∑–¥–∞–Ω–∏–µ AI –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `src/controllers/aiController.ts`:

```typescript
import axios from 'axios';
import { Request, Response } from 'express';

const OLLAMA_URL = process.env.OLLAMA_URL || 'http://localhost:11434';
const MODEL_NAME = process.env.OLLAMA_MODEL || 'llama3.2:1b';

interface ChatMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

// –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
const SYSTEM_PROMPT = `–¢—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç CashStream. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –ø–æ–º–æ–≥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –æ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö, –±—é–¥–∂–µ—Ç–µ, —Ä–∞—Å—Ö–æ–¥–∞—Ö –∏ –¥–æ—Ö–æ–¥–∞—Ö. 
–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –¥—Ä—É–∂–µ–ª—é–±–Ω–æ –∏ –ø–æ –¥–µ–ª—É. –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.`;

export const chatWithAI = async (req: Request, res: Response) => {
  try {
    const { message, conversationHistory = [] } = req.body;

    if (!message || typeof message !== 'string') {
      return res.status(400).json({
        success: false,
        message: 'Message is required'
      });
    }

    // –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π
    const messages: ChatMessage[] = [
      { role: 'system', content: SYSTEM_PROMPT },
      ...conversationHistory.map((msg: any) => ({
        role: msg.isUser ? 'user' : 'assistant',
        content: msg.text
      })),
      { role: 'user', content: message }
    ];

    // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ Ollama
    const response = await axios.post(
      `${OLLAMA_URL}/api/chat`,
      {
        model: MODEL_NAME,
        messages: messages,
        stream: false, // –î–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ true
        options: {
          temperature: 0.7,
          top_p: 0.9,
        }
      },
      {
        timeout: 60000 // 60 —Å–µ–∫—É–Ω–¥ —Ç–∞–π–º–∞—É—Ç
      }
    );

    const aiResponse = response.data.message.content;

    res.json({
      success: true,
      response: aiResponse
    });

  } catch (error: any) {
    console.error('AI Chat Error:', error);
    
    if (error.code === 'ECONNREFUSED') {
      return res.status(503).json({
        success: false,
        message: 'Ollama service is not running. Please start it with: ollama serve'
      });
    }

    res.status(500).json({
      success: false,
      message: 'Failed to get AI response',
      error: error.message
    });
  }
};

// –î–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ (–ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–∞—è –ø–µ—á–∞—Ç—å)
export const streamChatWithAI = async (req: Request, res: Response) => {
  try {
    const { message, conversationHistory = [] } = req.body;

    if (!message || typeof message !== 'string') {
      return res.status(400).json({
        success: false,
        message: 'Message is required'
      });
    }

    const messages: ChatMessage[] = [
      { role: 'system', content: SYSTEM_PROMPT },
      ...conversationHistory.map((msg: any) => ({
        role: msg.isUser ? 'user' : 'assistant',
        content: msg.text
      })),
      { role: 'user', content: message }
    ];

    // –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Å—Ç—Ä–∏–º–∏–Ω–≥
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');

    const response = await axios.post(
      `${OLLAMA_URL}/api/chat`,
      {
        model: MODEL_NAME,
        messages: messages,
        stream: true,
        options: {
          temperature: 0.7,
        }
      },
      {
        responseType: 'stream',
        timeout: 120000
      }
    );

    response.data.on('data', (chunk: Buffer) => {
      const lines = chunk.toString().split('\n').filter((line: string) => line.trim() !== '');
      
      for (const line of lines) {
        if (line.startsWith('data: ')) {
          try {
            const data = JSON.parse(line.slice(6));
            if (data.message?.content) {
              res.write(`data: ${JSON.stringify({ content: data.message.content })}\n\n`);
            }
            if (data.done) {
              res.write('data: [DONE]\n\n');
              res.end();
            }
          } catch (e) {
            // –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
          }
        }
      }
    });

    response.data.on('end', () => {
      res.end();
    });

    response.data.on('error', (error: Error) => {
      console.error('Stream error:', error);
      res.end();
    });

  } catch (error: any) {
    console.error('AI Stream Error:', error);
    if (!res.headersSent) {
      res.status(500).json({
        success: false,
        message: 'Failed to stream AI response'
      });
    }
  }
};
```

#### –®–∞–≥ 5: –°–æ–∑–¥–∞–Ω–∏–µ —Ä–æ—É—Ç–æ–≤

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `src/routes/aiRoutes.ts`:

```typescript
import { Router } from 'express';
import { chatWithAI, streamChatWithAI } from '../controllers/aiController';
import { protect } from '../middleware/auth';

const router = Router();

// –û–±—ã—á–Ω—ã–π —á–∞—Ç (–Ω–µ —Å—Ç—Ä–∏–º–∏–Ω–≥)
router.post('/chat', protect, chatWithAI);

// –°—Ç—Ä–∏–º–∏–Ω–≥ —á–∞—Ç (–¥–ª—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–π –ø–µ—á–∞—Ç–∏)
router.post('/chat/stream', protect, streamChatWithAI);

export default router;
```

#### –®–∞–≥ 6: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Ä–æ—É—Ç–æ–≤ –≤ `src/index.ts`

```typescript
// –î–æ–±–∞–≤—å—Ç–µ –∏–º–ø–æ—Ä—Ç
import aiRoutes from './routes/aiRoutes';

// –î–æ–±–∞–≤—å—Ç–µ —Ä–æ—É—Ç (–ø–æ—Å–ª–µ –¥—Ä—É–≥–∏—Ö API —Ä–æ—É—Ç–æ–≤)
app.use('/api/ai', aiRoutes);
```

#### –®–∞–≥ 7: –î–æ–±–∞–≤—å—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –≤ `.env`

```env
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:1b
```

---

### –í–∞—Ä–∏–∞–Ω—Ç 3: OpenAI API (–û–±–ª–∞—á–Ω—ã–π —Å–µ—Ä–≤–∏—Å)

–ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å –±–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏.

#### –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
yarn add openai
```

#### –®–∞–≥ 2: –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞

```typescript
import OpenAI from 'openai';
import { Request, Response } from 'express';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const SYSTEM_PROMPT = `–¢—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç CashStream...`;

export const chatWithAI = async (req: Request, res: Response) => {
  try {
    const { message, conversationHistory = [] } = req.body;

    const messages: any[] = [
      { role: 'system', content: SYSTEM_PROMPT },
      ...conversationHistory.map((msg: any) => ({
        role: msg.isUser ? 'user' : 'assistant',
        content: msg.text
      })),
      { role: 'user', content: message }
    ];

    const completion = await openai.chat.completions.create({
      model: 'gpt-3.5-turbo', // –∏–ª–∏ 'gpt-4' –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
      messages: messages,
      temperature: 0.7,
    });

    res.json({
      success: true,
      response: completion.choices[0].message.content
    });

  } catch (error: any) {
    console.error('OpenAI Error:', error);
    res.status(500).json({
      success: false,
      message: 'Failed to get AI response'
    });
  }
};
```

#### –®–∞–≥ 3: –î–æ–±–∞–≤—å—Ç–µ –≤ `.env`

```env
OPENAI_API_KEY=your-api-key-here
```

---

### –í–∞—Ä–∏–∞–Ω—Ç 4: –ü—Ä—è–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å llama.cpp (–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π)

–î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –º–æ–¥–µ–ª—å—é.

#### –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ llama-cpp-node

```bash
yarn add llama-cpp-node
```

#### –®–∞–≥ 2: –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å

–°–∫–∞—á–∞–π—Ç–µ GGUF –º–æ–¥–µ–ª—å —Å https://huggingface.co/models

#### –®–∞–≥ 3: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```typescript
import { LLama } from 'llama-cpp-node';

// –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (–¥–µ–ª–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ)
const llama = new LLama({
  modelPath: './models/llama-3.2-1b.Q4_K_M.gguf',
  nCtx: 2048,
  nGpuLayers: 0, // 0 –¥–ª—è CPU, –±–æ–ª—å—à–µ –¥–ª—è GPU
});

export const chatWithAI = async (req: Request, res: Response) => {
  const { message } = req.body;
  
  const response = await llama.createCompletion({
    prompt: `System: ${SYSTEM_PROMPT}\nUser: ${message}\nAssistant:`,
    nPredict: 512,
    temperature: 0.7,
  });

  res.json({
    success: true,
    response: response.text
  });
};
```

---

## üì± –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ –º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ

–ü–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–µ–∫–µ–Ω–¥–∞, –æ–±–Ω–æ–≤–∏—Ç–µ `src/api/operations.ts` –∏–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ `src/api/ai.ts`:

```typescript
import { apiClient } from './client';

export interface ChatMessage {
  text: string;
  isUser: boolean;
  timestamp: Date;
}

export interface ChatRequest {
  message: string;
  conversationHistory?: ChatMessage[];
}

export interface ChatResponse {
  success: boolean;
  response: string;
}

export const aiApi = {
  chat: async (data: ChatRequest): Promise<ChatResponse> => {
    const response = await apiClient.post<ChatResponse>('/ai/chat', data);
    return response.data;
  },
};
```

–ó–∞—Ç–µ–º –æ–±–Ω–æ–≤–∏—Ç–µ `AIChatScreen.tsx`:

```typescript
import { aiApi } from '../api/ai';

const handleSend = async () => {
  // ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
  
  setIsLoading(true);
  
  try {
    const response = await aiApi.chat({
      message: inputText.trim(),
      conversationHistory: messages.map(msg => ({
        text: msg.text,
        isUser: msg.isUser,
        timestamp: msg.timestamp
      }))
    });
    
    const aiMessage: Message = {
      id: (Date.now() + 1).toString(),
      text: response.response,
      isUser: false,
      timestamp: new Date(),
      isTyping: true,
    };
    
    setMessages(prev => [...prev, aiMessage]);
    
    // –ù–∞—á–∏–Ω–∞–µ–º —ç—Ñ—Ñ–µ–∫—Ç –ø–µ—á–∞—Ç–∏
    typewriterEffect(aiMessage.id, response.response, 30);
    
  } catch (error) {
    console.error('AI Chat Error:', error);
    // –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
  } finally {
    setIsLoading(false);
  }
};
```

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç (Docker - –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

1. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Docker:**
   ```bash
   # macOS
   brew install --cask docker
   ```

2. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä:**
   ```bash
   cd backendCashStream
   docker-compose up -d
   ```

3. **–ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å:**
   ```bash
   docker exec -it cashstream-ollama ollama pull llama3.2:1b
   ```

4. **–î–æ–±–∞–≤—å—Ç–µ –∫–æ–¥ –≤ –±–µ–∫–µ–Ω–¥** (—Å–º. –≤—ã—à–µ)

5. **–î–æ–±–∞–≤—å—Ç–µ —Ä–æ—É—Ç –≤ `src/index.ts`:**
   ```typescript
   import aiRoutes from './routes/aiRoutes';
   app.use('/api/ai', aiRoutes);
   ```

6. **–û–±–Ω–æ–≤–∏—Ç–µ –º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ** (—Å–º. –≤—ã—à–µ)

7. **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ:**
   ```bash
   curl -X POST http://localhost:3000/api/ai/chat \
     -H "Authorization: Bearer YOUR_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"message": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –º–Ω–µ —Å—ç–∫–æ–Ω–æ–º–∏—Ç—å –¥–µ–Ω—å–≥–∏?"}'
   ```

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç (–õ–æ–∫–∞–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞)

1. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama:**
   ```bash
   brew install ollama
   ```

2. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama:**
   ```bash
   ollama serve
   ```

3. **–ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å:**
   ```bash
   ollama pull llama3.2:1b
   ```

4. **–î–æ–±–∞–≤—å—Ç–µ –∫–æ–¥ –≤ –±–µ–∫–µ–Ω–¥** (—Å–º. –≤—ã—à–µ)

5. **–î–æ–±–∞–≤—å—Ç–µ —Ä–æ—É—Ç –≤ `src/index.ts`:**
   ```typescript
   import aiRoutes from './routes/aiRoutes';
   app.use('/api/ai', aiRoutes);
   ```

6. **–û–±–Ω–æ–≤–∏—Ç–µ –º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ** (—Å–º. –≤—ã—à–µ)

7. **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ:**
   ```bash
   curl -X POST http://localhost:3000/api/ai/chat \
     -H "Authorization: Bearer YOUR_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"message": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –º–Ω–µ —Å—ç–∫–æ–Ω–æ–º–∏—Ç—å –¥–µ–Ω—å–≥–∏?"}'
   ```

---

## üìä –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–æ–¥–µ–ª—è–º

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä | –°–∫–æ—Ä–æ—Å—Ç—å | –ö–∞—á–µ—Å—Ç–≤–æ | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|--------|--------|----------|----------|--------------|
| llama3.2:1b | ~1.3GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è |
| llama3.2:3b | ~2GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | –ë–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏/–∫–∞—á–µ—Å—Ç–≤–∞ |
| mistral:7b | ~4.1GB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞ |

---

## üîß Troubleshooting

### Ollama –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è
```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, –∑–∞–ø—É—â–µ–Ω –ª–∏ Ollama
curl http://localhost:11434/api/tags

# –ï—Å–ª–∏ –Ω–µ—Ç, –∑–∞–ø—É—Å—Ç–∏—Ç–µ
ollama serve
```

### –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
ollama list

# –ï—Å–ª–∏ –Ω—É–∂–Ω–æ–π –Ω–µ—Ç, –∑–∞–≥—Ä—É–∑–∏—Ç–µ
ollama pull llama3.2:1b
```

### –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å (1b –∏–ª–∏ 3b)
- –£–º–µ–Ω—å—à–∏—Ç–µ `nPredict` –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ

---

## ‚úÖ –ì–æ—Ç–æ–≤–æ!

–¢–µ–ø–µ—Ä—å –≤–∞—à AI —á–∞—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ Llama! üéâ

